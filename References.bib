@inproceedings{Gupta2019,
abstract = {Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced 'el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect 2.2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www.lvisdataset.org.},
archivePrefix = {arXiv},
arxivId = {1908.03195},
author = {Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00550},
eprint = {1908.03195},
file = {::},
isbn = {9781728132938},
issn = {10636919},
keywords = {Categorization,Datasets and Evaluation,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation},
month = {aug},
pages = {5351--5359},
title = {{Lvis: A dataset for large vocabulary instance segmentation}},
url = {http://arxiv.org/abs/1908.03195},
volume = {2019-June},
year = {2019}
}
@article{Dai2021,
abstract = {Spot welds are extensively used on autobodies and also a key factor affecting the performance of automobiles. Automatic detection of spot welding based on machine vision provides an effective way for car body welding quality control. Considering the traditional image processing methods are greatly disturbed by the environment and have unsatisfying robustness, a network model for small object detection is proposed to detect the position and quality of the spot welding of the car body. Based on the existing You Only Look Once (YOLOv3) model, the proposed model has three novel improvements. Firstly, the lightweight network MobileNetV3 is introduced to replace the backbone network of YOLOv3 to ensure accuracy and real-time performance. Secondly, to improve the model's ability for small object detection, a new feature pyramid network (FPN) with efÔ¨Åcient cross-scale connections is proposed, which allows easy and fast multiscale feature fusion. Finally, considering the shortcomings of intersection and union ratio (IoU) loss, complete IoU (CIoU) loss is used to improve convergence speed and regression accuracy. Moreover, novel data augmentation is used to enrich the dataset during the model training. Quantitative results on the spot welding dataset show that the proposed approach achieves successful results for resistance spot welding vision inspection.},
author = {Dai, Wei and Li, Dayong and Tang, Ding and Jiang, Qin and Wang, Dong and Wang, Huamiao and Peng, Yinghong},
doi = {https://doi.org/10.1016/j.jmapro.2020.12.015},
issn = {1526-6125},
journal = {Journal of Manufacturing Processes},
keywords = {Deep learning model,Quality vision inspection,Resistance spot welding,Small object detection,YOLOv3},
pages = {262--274},
title = {{Deep learning assisted vision inspection of resistance spot welds}},
url = {https://www.sciencedirect.com/science/article/pii/S1526612520308513},
volume = {62},
year = {2021}
}
@article{Quintana2012,
author = {Quintana, Andr{\'{e}}s Felipe Loaiza and Herrera, David Andr{\'{e}}s Manzano and Salazar, Luis Eduardo M{\'{u}}nera},
issn = {0121-0777},
journal = {El hombre y la m{\'{a}}quina},
number = {40},
pages = {87--101},
publisher = {Universidad Aut{\'{o}}noma de Occidente},
title = {{Sistema de visi{\'{o}}n artificial para conteo de objetos en movimiento}},
year = {2012}
}
@article{Sharma2020,
abstract = {Object detection can be regarded as one of the most fundamental and challenging visual recognition task in computer vision and it has received great attention over the past few decades. Object detection techniques find their application in almost all the spheres of life, most prominent ones being surveillance, autonomous driving, pedestrian detection and so on. The primary focus of visual object detection is to detect objects belonging to certain class targets with absolute localization in a realistic scene or an input image and also to assign each detected instance of an object a predefined class label. Owing to rapid development of deep neural networks, the performance of object detectors has rapidly improved and as a result of this deep learning based detection techniques have been actively studied over the past several years. In this paper we provide a comprehensive survey of latest advances in deep learning based visual object detection. Firstly we have reviewed a large body of recent works in literature and using that we have analyzed traditional and current object detectors. Afterwards and primarily we provide a rigorous overview of backbone architectures for object detection followed by a systematic cover up of current learning strategies. Some popular datasets and metrics used for object detection are analyzed as well. Finally we discuss applications of object detection and provide several future directions to facilitate future research for visual object detection with deep learning.},
author = {Sharma, Vipul and Mir, Roohie Naaz},
doi = {https://doi.org/10.1016/j.cosrev.2020.100301},
issn = {1574-0137},
journal = {Computer Science Review},
keywords = {Classification,Convolutional neural networks,Deep learning,Localization,Object detection,Segmentation},
pages = {100301},
title = {{A comprehensive and systematic look up into deep learning based object detection techniques: A review}},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304019},
volume = {38},
year = {2020}
}
@article{Zamora-Hernandez2021,
abstract = {Product assembly is a crucial process in manufacturing plants. In Industry 4.0, the offer of mass-customized products is expanded, thereby increasing the complexity of the assembling phase. This implies that operators should pay close attention to small details, potentially resulting in errors during the manufacturing process owing to its high level of complexity. To mitigate this, we propose a novel architecture that evaluates the activities of an operator during manual assembly in a production cell so that errors in the manufacturing process can be identified, thus avoiding low quality in the final product and reducing rework and waste of raw materials or time. To perform this assessment, it is necessary to use state-of-the-art computer vision techniques, such as deep learning, so that tools, components, and actions may be identified by visual control systems. We develop a deep-learning-based visual control assembly assistant that enables real-time evaluation of the activities in the assembly process so that errors can be identified. A general-use language is developed to describe the actions in assembly processes, which can also be used independently of the proposed architecture. Finally, we generate two datasets with annotated data to be fed to the deep learning methods, the first for the recognition of tools and accessories and the second for the identification of basic actions in manufacturing processes. To validate the proposed method, a set of experiments are conducted, and high accuracy is obtained.},
author = {Zamora-Hern{\'{a}}ndez, Mauricio-Andr{\'{e}}s and Castro-Vargas, John Alejandro and Azorin-Lopez, Jorge and Garcia-Rodriguez, Jose},
doi = {https://doi.org/10.1016/j.compind.2021.103485},
issn = {0166-3615},
journal = {Computers in Industry},
pages = {103485},
title = {{Deep learning-based visual control assistant for assembly in Industry 4.0}},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000920},
volume = {131},
year = {2021}
}
@article{Yun2020,
abstract = {Recent efforts to create a smart factory have inspired research that analyzes process data collected from Internet of Things (IOT) sensors, to predict product quality in real time. This requires an automatic defect inspection system that quantifies product quality data by detecting and classifying defects in real time. In this study, we propose a vision-based defect inspection system to inspect metal surface defects. In recent years, deep convolutional neural networks (DCNNs) have been used in many manufacturing industries and have demonstrated the excellent performance as a defect classification method. A sufficient amount of training data must be acquired, to ensure high performance using a DCNN. However, owing to the nature of the metal manufacturing industry, it is difficult to obtain enough data because some defects occur rarely. Owing to this imbalanced data problem, the generalization performance of the DCNN-based classification algorithm is lowered. In this study, we propose a new convolutional variational autoencoder (CVAE) and deep CNN-based defect classification algorithm to solve this problem. The CVAE-based data generation technology generates sufficient defect data to train the classification model. A conditional CVAE (CCVAE) is proposed to generate images for each defect type in a single CVAE model. We also propose a classifier based on a DCNN with high generalization performance using data generated from the CCVAE. In order to verify the performance of the proposed method, we performed experiments using defect images obtained from an actual metal production line. The results showed that the proposed method exhibited an excellent performance.},
author = {Yun, Jong Pil and Shin, Woosang Crino and Koo, Gyogwon and Kim, Min Su and Lee, Chungki and Lee, Sang Jun},
doi = {https://doi.org/10.1016/j.jmsy.2020.03.009},
issn = {0278-6125},
journal = {Journal of Manufacturing Systems},
keywords = {Deep learning,Defect detection,Machine learning,Metal surfaces,Vision inspection},
pages = {317--324},
title = {{Automated defect inspection system for metal surfaces based on deep learning and data augmentation}},
url = {https://www.sciencedirect.com/science/article/pii/S027861252030042X},
volume = {55},
year = {2020}
}
@article{Carvalho2021,
abstract = {Quality management practices are widely implemented by companies, as they constitute a competitive advantage. Nowadays it is almost mandatory to follow quality standards, in order to make a product available on the market. However, facing new production paradigms, such as Industry 4.0, questions arise about how quality management processes could benefit and adapt in the era of digital technologies. Following a literature review approach, this paper lead to the development of a table that links the relationship between quality management practices and Industry 4.0 technologies that improve quality, as it aimed.},
author = {Carvalho, Adriana Ventura and Enrique, Daisy Valle and Chouchene, Amal and Charrua-Santos, Fernando},
doi = {https://doi.org/10.1016/j.procs.2021.01.176},
issn = {1877-0509},
journal = {Procedia Computer Science},
keywords = {Industry 4.0,digital technologies,quality 4.0,quality management},
pages = {341--346},
title = {{Quality 4.0: An Overview}},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002167},
volume = {181},
year = {2021}
}
